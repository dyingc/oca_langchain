# 请将以下值替换为您的真实凭证和配置

# --- OAuth2 认证配置 ---
OAUTH_HOST=""
OAUTH_CLIENT_ID=""
OAUTH_REFRESH_TOKEN=""

# --- LLM API 配置 ---
# 这是您的语言模型API的端点URL (例如: https://api.your-provider.com/v1/chat/completions)
LLM_API_URL=""

# 要使用的模型名称
LLM_MODEL_NAME=""

# 获取可用模型列表的 API 端点
LLM_MODELS_API_URL=""

# 默认的系统提示 (System Prompt)
LLM_SYSTEM_PROMPT="You are a helpful assistant."

# 默认的采样温度 (0.0 到 2.0 之间)
LLM_TEMPERATURE="0.7"

# --- 网络配置 ---
# 设置为 "true" 时，始终通过 HTTP_PROXY_URL 发送请求；默认直连优先加自动用代理
FORCE_PROXY="false"
# 如果应用无法直接访问 OAuth 或 LLM API，请在此处指定 HTTP 代理服务器的地址
# 例如: http://user:password@proxy.example.com:8080
HTTP_PROXY_URL=""

# 附加 CA 证书列表（多个 PEM 用逗号分隔，可为空）
MULTI_CA_BUNDLE=""

# 可选：自定义 CA bundle 路径（若设置则优先生效）
# REQUESTS_CA_BUNDLE=""


# 网络连接超时时间 (单位: 秒，支持浮点数，建议2秒及以上。可选)
CONNECTION_TIMEOUT="2"

# LLM 请求的超时时间 (单位: 秒，支持浮点数，建议 60 秒及以上。可选)
LLM_REQUEST_TIMEOUT="180"

# --- 日志配置 ---
# LOG_LEVEL supports: DEBUG and INFO (default INFO)
LOG_LEVEL="INFO"
# LOG_FILE_PATH can be absolute or relative; parent folders will be auto-created.
# It can include subfolders, e.g. logs/llm_api.log
LOG_FILE_PATH="logs/llm_api.log"
