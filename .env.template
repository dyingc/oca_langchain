# 请将以下值替换为您的真实凭证和配置

# --- OAuth2 认证配置 ---
OAUTH_HOST=""
OAUTH_CLIENT_ID=""
OAUTH_REFRESH_TOKEN=""

# --- LLM API 配置 ---
# 这是您的语言模型API的端点URL (例如: https://api.your-provider.com/v1/chat/completions)
LLM_API_URL=""

# 要使用的模型名称
LLM_MODEL_NAME=""

# 获取可用模型列表的 API 端点
LLM_MODELS_API_URL=""

# Responses API 端点 (可选，用于透传 OpenAI Responses API 格式的请求)
# 如果后端 LLM 原生支持 Responses API，设置此 URL 可直接透传请求
# 例如: https://api.your-provider.com/v1/responses
LLM_RESPONSE_API_URL=""

# Reasoning 配置 (可选，用于透传时覆盖请求中的 reasoning.effort)
# 有效值: low, medium, high, xhigh, minimal, none
# 当请求中已有 reasoning.effort 时，使用此值覆盖
LLM_REASONING_STRENGTH=""
# 当请求中 reasoning 为 null 时，使用此值构建 reasoning 对象
LLM_NON_REASONING_STRENGTH=""

# 默认的系统提示 (System Prompt)
LLM_SYSTEM_PROMPT="You are a helpful assistant."

# 默认的采样温度 (0.0 到 2.0 之间)
LLM_TEMPERATURE="0.7"

# --- 网络配置 ---
# 设置为 "true" 时，始终通过 HTTP_PROXY_URL 发送请求；默认直连优先加自动用代理
FORCE_PROXY="false"
# 如果应用无法直接访问 OAuth 或 LLM API，请在此处指定 HTTP 代理服务器的地址
# 例如: http://user:password@proxy.example.com:8080
HTTP_PROXY_URL=""

# 附加 CA 证书列表（多个 PEM 用逗号分隔，可为空）
MULTI_CA_BUNDLE=""
# 是否全局禁用 SSL 证书校验（true/false），仅建议在调试或 MITM 代理场景使用
DISABLE_SSL_VERIFY="false"

# 可选：自定义 CA bundle 路径（若设置则优先生效）
# REQUESTS_CA_BUNDLE=""


# 网络连接超时时间 (单位: 秒，支持浮点数，建议2秒及以上。可选)
CONNECTION_TIMEOUT="2"

# LLM 请求的超时时间 (单位: 秒，支持浮点数，建议 60 秒及以上。可选)
LLM_REQUEST_TIMEOUT="180"

# --- 日志配置 ---
# LOG_LEVEL supports: DEBUG and INFO (default INFO)
LOG_LEVEL="INFO"
# LOG_FILE_PATH can be absolute or relative; parent folders will be auto-created.
# It can include subfolders, e.g. logs/llm_api.log
LOG_FILE_PATH="logs/llm_api.log"
